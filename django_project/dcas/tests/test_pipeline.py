# coding=utf-8
"""
Tomorrow Now GAP.

.. note:: Unit tests for DCAS Pipeline.
"""

# import uuid
# import os
from mock import patch, MagicMock
import pandas as pd
import dask.dataframe as dd
# from django.test import TransactionTestCase
from sqlalchemy import create_engine

# from gap.models import Crop, CropStageType
from dcas.models import DCASConfig, DCASConfigCountry
from dcas.pipeline import DCASDataPipeline
# from dcas.outputs import OutputType
from dcas.tests.base import DCASPipelineBaseTest
# from dcas.utils import read_grid_crop_data


def mock_function_do_nothing(df, *args, **kwargs):
    """Mock dask map_partitions do nothing."""
    return df


class DCASPipelineTest(DCASPipelineBaseTest):
    """DCAS Pipeline test case."""

    def test_merge_grid_data_with_config(self):
        """Test merge_grid_data_with_config."""
        id_list = [self.grid_1.id, self.grid_2.id]
        country_id_list = [
            self.grid_1.country.id,
            self.grid_2.country.id,
        ]
        df = pd.DataFrame({
            'country_id': country_id_list
        }, index=id_list)
        # create config for country 2
        config = DCASConfig.objects.create(name='config')
        DCASConfigCountry.objects.create(
            config=config,
            country=self.grid_2.country
        )

        pipeline = DCASDataPipeline(
            [self.farm_registry_group.id], self.request_date
        )
        df = pipeline._merge_grid_data_with_config(df)
        self.assertIn('config_id', df.columns)
        expected_df = pd.DataFrame({
            'country_id': country_id_list,
            'config_id': [self.default_config.id, config.id]
        }, index=df.index)
        expected_df['config_id'] = expected_df['config_id'].astype('Int64')
        pd.testing.assert_frame_equal(df, expected_df)

    def test_merge_grid_data_with_config_using_default(self):
        """Test merge_grid_data_with_config using default."""
        id_list = [self.grid_1.id, self.grid_2.id]
        country_id_list = [
            self.grid_1.country.id,
            self.grid_2.country.id,
        ]
        df = pd.DataFrame({
            'country_id': country_id_list
        }, index=id_list)
        pipeline = DCASDataPipeline(
            [self.farm_registry_group.id], self.request_date
        )
        df = pipeline._merge_grid_data_with_config(df)
        self.assertIn('config_id', df.columns)
        expected_df = pd.DataFrame({
            'country_id': country_id_list,
            'config_id': [self.default_config.id, self.default_config.id]
        }, index=df.index)
        expected_df['config_id'] = expected_df['config_id'].astype('Int64')
        pd.testing.assert_frame_equal(df, expected_df)

    def test_process_farm_registry_data(self):
        """Test running process_farm_registry_data."""
        pipeline = DCASDataPipeline(
            [self.farm_registry_group.id], self.request_date
        )
        conn_engine = create_engine(pipeline._conn_str())
        pipeline.data_query.setup(conn_engine)

        grid_crop_meta_df = pd.DataFrame({
            'crop_id': [1],
            'crop_stage_type_id': [1],
            'planting_date': ['2025-01-01'],
            'grid_id': [1],
            'planting_date_epoch': [1],
            '__null_dask_index__': [0],
            'temperature': [10],
            'grid_crop_key': ['1_1_1']
        })
        pipeline.data_query.read_grid_data_crop_meta_parquet = MagicMock(
            return_value=grid_crop_meta_df
        )
        pipeline.data_output.save = MagicMock()

        # Mock the map_partitions method
        with patch.object(
            dd.DataFrame, 'map_partitions', autospec=True
        ) as mock_map_partitions:
            # Set up the mock to call the mock function
            mock_map_partitions.side_effect = mock_function_do_nothing

            pipeline.process_farm_registry_data()

            mock_map_partitions.assert_called_once()

        pipeline.data_query.read_grid_data_crop_meta_parquet.\
            assert_called_once()
        pipeline.data_output.save.assert_called_once()
        conn_engine.dispose()

    @patch("dask.dataframe.read_parquet", return_value=MagicMock())
    @patch("dask.dataframe.DataFrame.to_parquet")  # Mock to_parquet
    def test_filter_message_output(self, mock_to_parquet, mock_read_parquet):
        """Ensure `df.to_parquet()` is executed for Codecov coverage."""
        test_data = {
            "grid_id": [1, 2],
            "crop_id": [100, 200],
            "message": ["msg1", "msg2"],
        }
        expected_df = dd.from_pandas(pd.DataFrame(test_data), npartitions=1)
        mock_read_parquet.return_value = expected_df  # Mock read_parquet

        # Ensure `to_parquet()` is a callable mock
        mock_to_parquet.return_value = None

        # Initialize Pipeline
        pipeline = DCASDataPipeline([1], "2025-01-01")

        pipeline.data_output.setup()

        # Call Function (No Assertions Needed)
        pipeline.filter_message_output()
        expected_df.compute()

        # Ensure `df.to_parquet()` was called
        mock_to_parquet.assert_called_once()


# class DCASAllPipelineTest(TransactionTestCase, BasePipelineTest):
#     """Test to run the pipeline with committed transaction."""

#     fixtures = [
#         '2.provider.json',
#         '3.station_type.json',
#         '4.dataset_type.json',
#         '5.dataset.json',
#         '6.unit.json',
#         '7.attribute.json',
#         '8.dataset_attribute.json',
#         '1.dcas_config.json',
#         '12.crop_stage_type.json',
#         '13.crop_growth_stage.json',
#         '14.crop.json',
#         '15.gdd_config.json',
#         '16.gdd_matrix.json'
#     ]

#     def setUp(self):
#         """Set DCASAllPipelineTest class."""
#         self.setup_test()
#         self._ingest_rule()

#         self.pipeline = DCASDataPipeline(
#             self.farm_registry_group, self.request_date
#         )
#         self.pipeline.GRID_CROP_NUM_PARTITIONS = 1
#         self.pipeline.data_output.TMP_BASE_DIR = f'/tmp/{uuid.uuid4().hex}'
#         self.pipeline.setup()

#     def tearDown(self):
#         """Clean test resources."""
#         self.pipeline.cleanup()

#     def test_process_grid_crop_data(self):
#         """Test process_grid_crop_data."""
#         grid_data = {
#             'grid_id': [self.grid_1.id, self.grid_2.id],
#             'config_id': [1, 1],
#             'temperature': [10, 12],
#             'humidity': [13, 14],
#             'p_pet': [15, 16],
#             'total_precipitation': [17, 18],
#             'total_evapotranspiration': [19, 20]
#         }
#         for epoch in self.pipeline.data_input.historical_epoch:
#             grid_data[f'max_temperature_{epoch}'] = [21, 22]
#             grid_data[f'min_temperature_{epoch}'] = [23, 24]
#             grid_data[f'total_rainfall_{epoch}'] = [25, 26]

#         grid_df = pd.DataFrame(grid_data)
#         self.pipeline.data_output.save(OutputType.GRID_DATA, grid_df)

#         self.pipeline.process_grid_crop_data()

#         self.assertTrue(
#             os.path.exists(self.pipeline.data_output.grid_crop_data_dir_path)
#         )

#         cassava_id = Crop.objects.get(name='Cassava').id
#         early_id = CropStageType.objects.get(name='Early').id
#         df = read_grid_crop_data(
#             self.pipeline.data_output.grid_crop_data_path,
#             [
#                 f'{cassava_id}_{early_id}_{self.grid_1.id}',
#                 f'{cassava_id}_{early_id}_{self.grid_2.id}'
#             ]
#         )
#         self.assertEqual(df.shape[0], 2)
#         self.assertIn('message', df.columns)
#         self.assertIn('message_2', df.columns)
#         self.assertIn('message_3', df.columns)
#         self.assertIn('message_4', df.columns)
#         self.assertIn('message_5', df.columns)
#         self.assertIn('total_gdd', df.columns)
#         self.assertIn('temperature', df.columns)
#         self.assertIn('p_pet', df.columns)
#         self.assertIn('humidity', df.columns)
#         self.assertIn('seasonal_precipitation', df.columns)
#         self.assertIn('growth_stage_precipitation', df.columns)
#         self.assertIn('grid_id', df.columns)
#         self.assertIn('crop_id', df.columns)
#         self.assertIn('planting_date', df.columns)
#         self.assertIn('growth_stage_id', df.columns)

#         pd.testing.assert_series_equal(
#             df['total_gdd'],
#             pd.Series([60, 66], name='total_gdd', dtype='float64')
#         )
#         pd.testing.assert_series_equal(
#             df['p_pet'],
#             pd.Series([15, 16], name='p_pet', dtype='float64')
#         )
#         pd.testing.assert_series_equal(
#             df['temperature'],
#             pd.Series([10, 12], name='temperature', dtype='float64')
#         )
#         pd.testing.assert_series_equal(
#             df['humidity'],
#             pd.Series([13, 14], name='humidity', dtype='float64')
#         )
#         pd.testing.assert_series_equal(
#             df['seasonal_precipitation'],
#             pd.Series(
#                 [250, 260], name='seasonal_precipitation',
#                 dtype='float64'
#             )
#         )
#         pd.testing.assert_series_equal(
#             df['growth_stage_precipitation'],
#             pd.Series(
#                 [125, 130], name='growth_stage_precipitation',
#                 dtype='float64'
#             )
#         )
